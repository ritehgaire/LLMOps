{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b6235c-114b-4ec8-91ac-afd5563e8b5a",
   "metadata": {},
   "source": [
    "Ensuring Prediction Accuracy and Safety in Production\n",
    "In a production environment, it’s crucial to maintain both prediction accuracy and safety. One common approach is to deploy the model as a REST API, allowing real-time interaction where a user sends a request (e.g., a question), and the model returns a prediction. Batch processing can also be utilized for offline tasks like sending multiple review inputs to the model for predictions.\n",
    "\n",
    "When deploying via an API, frameworks like PyTorch and TensorFlow can be packaged with tools such as Flask, making the model accessible as a service. Load balancing can be achieved by randomly selecting models from multiple endpoints, ensuring that traffic is distributed across different deployed versions. A key principle in production is to stay as close as possible to the data used during training. This is typically done by maintaining the same format and structure for prompts, which ensures consistent model performance.\n",
    "\n",
    "Prediction Latency in Larger Models\n",
    "Larger models generally incur higher latency, which can impact user experience. It’s important to define permissible latency (e.g., is a 2-second delay acceptable?). Reducing latency can be achieved by using smaller models, faster hardware, or deploying models regionally to minimize network delays. Optimizing for response time is crucial for user satisfaction in real-time systems.\n",
    "\n",
    "Prompt Generation with Instruction and Question\n",
    "To align production inputs with training data, prompts in the same format used during training should be sent to the model. A typical prompt consists of:\n",
    "\n",
    "Instruction: Guides the model on how to respond.\n",
    "Question: Represents the user’s input.\n",
    "This structured format ensures that the model consistently understands the task, improving prediction reliability. Staying close to the format used in training minimizes errors and improves model performance.\n",
    "\n",
    "Safety Attributes and Thresholds\n",
    "Safety attributes are crucial in ensuring the model’s responses remain appropriate. Models can have built-in mechanisms to block unsafe content, and practitioners can define additional thresholds based on probability (likelihood of harmful content) and severity (level of harm if the content is inappropriate). This layered approach ensures that responses meet safety guidelines before being presented to users.\n",
    "\n",
    "Probability vs Severity Score\n",
    "Probability score: Reflects how likely a response contains unsafe content.\n",
    "Severity score: Indicates the potential impact of the content if it crosses the threshold.\n",
    "By setting thresholds for both scores, practitioners can control how responses are managed, balancing between acceptable and harmful outputs.\n",
    "\n",
    "Citation Tracking\n",
    "In production, ensuring the originality of content is key. Citation metadata helps track whether the model's response is derived from existing sources. This helps reduce the risk of plagiarism and ensures transparency in model outputs, particularly when the content needs to be factual or attributed.\n",
    "\n",
    "Beyond Deployment: Packaging, Versioning, and Monitoring\n",
    "Beyond deploying the model, key considerations include:\n",
    "\n",
    "Packaging and deployment: Ensuring the model is correctly bundled with dependencies.\n",
    "Versioning: Keeping track of different model versions for reproducibility and rollback capabilities.\n",
    "Monitoring: Continuously tracking model performance through metrics like accuracy and bias detection.\n",
    "Scalability: Ensuring the model can handle growing traffic with load testing and controlled rollouts.\n",
    "Latency management: Balancing response times with user experience requirements.\n",
    "Through regular monitoring and improvements, production models can be kept safe, efficient, and high-performing, ensuring consistent delivery of value to users.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699eeedf-67bc-473b-9ffc-990d61a41628",
   "metadata": {},
   "source": [
    "Real-Life Example: Predictions, Prompts, and Safety with PaLM 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a3a46-bc9c-4b75-86f7-8161c2cb557c",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to deploy a pre-trained model (PaLM 2), interact with it through REST API endpoints, and manage prompts and safety attributes. We'll load the project, initialize the model, send requests with different prompts, and evaluate the safety of the generated responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3df3cf6-10f5-4e27-bd11-6155bed1b8ad",
   "metadata": {},
   "source": [
    "Step 1: Load Project ID and Credentials\n",
    "Objective: Authenticate and initialize the project credentials to access Google Cloud resources and Vertex AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8feea7-0487-47b3-b919-9b4599244b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Project ID and credentials\n",
    "from utils import authenticate\n",
    "\n",
    "# Authenticate and get credentials\n",
    "credentials, PROJECT_ID = authenticate()\n",
    "\n",
    "# Set the region for model execution\n",
    "REGION = \"us-central1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2befb-6d21-479d-9c77-bbd4e7f4c741",
   "metadata": {},
   "source": [
    "Step 2: Initialize Vertex AI SDK and Load the Pre-Trained Model\n",
    "Objective: Import Vertex AI SDK, initialize the model with project details, and load the pre-trained PaLM 2 model (text-bison@001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b8765-673c-401b-a590-0e0295e43031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Vertex AI SDK and model class for text generation\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "# Initialize Vertex AI SDK with project, location, and credentials\n",
    "vertexai.init(project=PROJECT_ID,\n",
    "              location=REGION,\n",
    "              credentials=credentials)\n",
    "\n",
    "# Load the pre-trained model text-bison@001 from Vertex AI\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd97622-9668-4de7-aaba-e32a3cbff066",
   "metadata": {},
   "source": [
    "Step 3: Retrieve and List Deployed Models (Endpoints)\n",
    "Objective: Retrieve the list of deployed models (tuned versions of text-bison@001) and print the available models. This will allow load balancing between endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7834b05-8a46-4756-a7d3-fa20b1a5bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and list the names of all tuned models (endpoints)\n",
    "list_tuned_models = model.list_tuned_model_names()\n",
    "\n",
    "# Print the available tuned models\n",
    "for i in list_tuned_models:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bb65d-d859-4feb-97f2-cd9645f41801",
   "metadata": {},
   "source": [
    "Step 4: Randomly Select a Model for Load Balancing\n",
    "Objective: Randomly select one of the available models from the list for load balancing, ensuring that prediction traffic is distributed across different endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff427cf-fee9-48d3-bf74-88aab577643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import random module to select a model randomly\n",
    "import random\n",
    "\n",
    "# Randomly select one of the tuned models for prediction\n",
    "tuned_model_select = random.choice(list_tuned_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c246181-4de9-4409-a325-e3432b252f96",
   "metadata": {},
   "source": [
    "Step 5: Load the Selected Model and Define a Prompt\n",
    "Objective: Load the randomly selected model and define a prompt that matches the type of questions the model was trained on (e.g., Python-related questions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35869b7d-e5fc-4f6b-8aa3-d2493cdbb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the endpoint of the randomly selected model\n",
    "deployed_model = TextGenerationModel.get_tuned_model(tuned_model_select)\n",
    "\n",
    "# Define a Python-related question as the prompt for prediction\n",
    "PROMPT = \"How can I load a CSV file using Pandas?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0ba56-08dc-4276-80d0-c9bfd7a62a7c",
   "metadata": {},
   "source": [
    "Step 6: Send the Prompt and Get a Response\n",
    "Objective: Send the defined prompt to the deployed model and receive a response. The response might take some time depending on the latency of the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bde6a3-792c-4cd6-aaf3-24465117c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the prompt to the model and get the response\n",
    "response = deployed_model.predict(PROMPT)\n",
    "\n",
    "# Print the received response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c6b04-a333-4efd-a800-5a0da94ae92c",
   "metadata": {},
   "source": [
    "Step 7: Extract the First Object from the Response\n",
    "Objective: Extract and print the first object from the response to view the generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaf0533-0a2e-43db-bcd0-97208620e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pprint for easier readability of the response\n",
    "from pprint import pprint\n",
    "\n",
    "# Load the first object from the response (prediction)\n",
    "output = response._prediction_response[0]\n",
    "\n",
    "# Print the first object of the response\n",
    "pprint(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02341d92-bf04-43d8-a171-ca523277e38f",
   "metadata": {},
   "source": [
    "Step 8: Extract and Print the Content from the Response\n",
    "Objective: Dig deeper into the response and retrieve the \"content\" key, which contains the actual answer generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c2a32-437a-4edb-8160-2fb816feb242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"content\" key from the first object of the response\n",
    "final_output = response._prediction_response[0][0][\"content\"]\n",
    "\n",
    "# Print the final content from the response\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1406cd90-bb3e-4ba9-92b1-21594c42c36c",
   "metadata": {},
   "source": [
    "Step 9: Use a New Prompt with Instruction and Question\n",
    "Objective: To ensure consistency with the model's training data, combine the instruction and question into a new prompt and get a response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c68759-527a-464b-a286-9b6b0239124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the instruction that was used during training\n",
    "INSTRUCTION = \"\"\"\\\n",
    "Please answer the following Stackoverflow question on Python.\\\n",
    "Answer it like you are a developer answering Stackoverflow questions.\\\n",
    "Question:\n",
    "\"\"\"\n",
    "\n",
    "# Define the question\n",
    "QUESTION = \"How can I store my TensorFlow checkpoint on Google Cloud Storage? Python example?\"\n",
    "\n",
    "# Combine the instruction and question into a single prompt\n",
    "PROMPT = f\"\"\"\n",
    "{INSTRUCTION} {QUESTION}\n",
    "\"\"\"\n",
    "\n",
    "# Print the combined prompt\n",
    "print(PROMPT)\n",
    "\n",
    "# Send the new prompt to the model and get a response\n",
    "final_response = deployed_model.predict(PROMPT)\n",
    "\n",
    "# Extract the \"content\" from the response\n",
    "output = final_response._prediction_response[0][0][\"content\"]\n",
    "\n",
    "# Print the new output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd5ec1-15ff-4f9d-85c3-3ac8bbffbd74",
   "metadata": {},
   "source": [
    "Step 10: Check the Safety Attributes of the Response\n",
    "Objective: Verify if the model’s response was blocked by its internal safety mechanisms. Also, retrieve and print the safety attributes associated with the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd819024-f9c5-4189-a712-fc30e262bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the \"blocked\" key from the safetyAttributes of the response\n",
    "blocked = response._prediction_response[0][0]['safetyAttributes']['blocked']\n",
    "\n",
    "# Print whether the response was blocked (True/False)\n",
    "print(blocked)\n",
    "\n",
    "# Retrieve the entire safetyAttributes section from the response\n",
    "safety_attributes = response._prediction_response[0][0]['safetyAttributes']\n",
    "\n",
    "# Print the safety attributes to check the safety scores\n",
    "pprint(safety_attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bdd4d6-0e8c-45bd-b952-8e7754a88ada",
   "metadata": {},
   "source": [
    "Step 11: Check the Citation Metadata\n",
    "Objective: Check if the generated response includes any citations or external sources, ensuring that the response is original or properly referenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e74b9e-e84e-44e4-b1e5-16b785db4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the \"citations\" key from the citationMetadata of the response\n",
    "citation = response._prediction_response[0][0]['citationMetadata']['citations']\n",
    "\n",
    "# Print the citations (if any), or an empty list if none are found\n",
    "pprint(citation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33b3a6-399d-44f8-add7-5cfbc265f7e0",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "This notebook demonstrates how to deploy a pre-trained model from Vertex AI, interact with it via prompts, and manage safety attributes and citations. The ability to use REST API endpoints with load balancing ensures that the prediction load is distributed efficiently. Additionally, checking safety attributes and citations helps ensure the model’s response is both safe and original."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
