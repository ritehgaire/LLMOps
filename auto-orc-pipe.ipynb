{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75c093e-580d-4421-8fb3-7b859181a4a5",
   "metadata": {},
   "source": [
    "Model Tuning and Building a Machine Learning Pipeline\n",
    "In machine learning, model tuning is a crucial process that is often performed multiple times to optimize model performance. Building a machine learning (ML) pipeline using an open-source workflow facilitates this process, allowing for streamlined and repeatable model training and evaluation.\n",
    "\n",
    "MLOps Workflow\n",
    "An MLOps workflow integrates various stages of the machine learning lifecycle. During model development, we focus on three key components: training data, training and evaluation, and the resulting trained model. Once the model is deployed to production, we use production data alongside the trained model to make predictions and continue the evaluation process. Automating this MLOps workflow ensures continuous and efficient model improvement and deployment.\n",
    "\n",
    "Automation, Orchestration, and Deployment\n",
    "To achieve this level of automation, orchestration, automation, and deployment are critical components. Orchestration helps organize and automate multiple tasks in the pipeline, ensuring that each step (from data processing to model training) is executed in sequence. Tools like Airflow and Kubeflow are widely used for orchestrating ML pipelines. In this context:\n",
    "\n",
    "Orchestration involves coordinating different tasks.\n",
    "Automation ensures tasks are executed without manual intervention.\n",
    "Deployment makes the model available in production for real-world use.\n",
    "Components of Orchestration\n",
    "Orchestration in ML pipelines can be broken down into two primary components:\n",
    "\n",
    "Data Processing: The first component of the pipeline, responsible for collecting, cleaning, and transforming raw data into a suitable format for model training.\n",
    "Model Training: The second component, where the machine learning model is trained on the processed data and fine-tuned as needed.\n",
    "These components work together in an automated workflow, enabling continuous integration and deployment of machine learning models.\n",
    "\n",
    "DSL (Domain-Specific Language) for Pipelines\n",
    "DSL (Domain-Specific Language) is a set of instructions or syntax specifically designed for defining machine learning pipelines. In Kubeflow, DSL allows users to define and configure pipeline steps (such as data preprocessing and model training). Kubeflow uses DSL to translate these pipeline definitions into executable workflows.\n",
    "\n",
    "Kubeflow workflows operate within a containerized environment, where each step of the pipeline runs inside a container. A container includes not only the code but also all necessary dependencies and the operating system. This allows for scalability and portability, as containers can be run consistently across different environments without requiring users to manage servers, operating systems, or other infrastructure.\n",
    "\n",
    "Orchestration in Machine Learning\n",
    "In machine learning, orchestration is vital for automating tasks such as data processing, model training, and evaluation. URIs (Uniform Resource Identifiers) are often used to pass data by location, ensuring that different pipeline components can easily access the required data. Additionally, pipelines can be parameterized to enable reuse across different datasets and tasks, such as in supervised fine-tuning.\n",
    "\n",
    "Purpose of YAML File in Kubeflow Pipelines\n",
    "Kubeflow pipelines can be compiled into a YAML file, which is a human-readable format used for configuration. The purpose of generating a YAML file in a machine learning pipeline is to provide a declarative specification of the pipeline. This file defines the structure of the pipeline, including its components, inputs, and outputs.\n",
    "\n",
    "By using YAML files:\n",
    "\n",
    "Reusability is enhanced, as the pipeline can be re-executed with different parameters without modifying the core logic.\n",
    "Portability is achieved, as the pipeline configuration can be shared across environments.\n",
    "Scalability is ensured, as the pipeline steps are containerized and can run in distributed environments, without worrying about infrastructure details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257eaf82-bc85-40a7-bca1-9445de403d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kubeflow Pipelines Automation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161d452-6d0a-4c0f-ad0f-c31fac6af969",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Kubeflow Pipelines to orchestrate and automate a workflow. It walks through the process of defining components, building a pipeline, compiling it into a YAML file, and submitting it for execution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb669ba-4036-4e1a-bbe6-6843bdc0e2a2",
   "metadata": {},
   "source": [
    "Step 1: Import Necessary Libraries and Ignore Warnings\n",
    "The purpose of this cell is to import the required libraries for working with Kubeflow Pipelines and set up the environment. It also ignores future warnings to prevent clutter in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b0455-d0fc-426a-82f7-1f8cd916d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries from kfp\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "\n",
    "# Ignore FutureWarnings from Kubeflow Pipelines (kfp)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "                        category=FutureWarning, \n",
    "                        module='kfp.*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d93090b-d27a-4cb9-b32f-e344a16a17f7",
   "metadata": {},
   "source": [
    "Step 2: Define the First Pipeline Component\n",
    "This component (say_hello) is a simple function that takes a name and returns a greeting. The @dsl.component decorator makes it a pipeline component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1094e-1604-4c72-996e-5047c13407bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple example: component 1\n",
    "@dsl.component\n",
    "def say_hello(name: str) -> str:\n",
    "    hello_text = f'Hello, {name}!'  # Create a greeting message\n",
    "    return hello_text  # Return the greeting message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808ff888-04d6-4018-a250-75f8fa628c63",
   "metadata": {},
   "source": [
    "Step 3: Test the First Component\n",
    "Here, we call the say_hello component and print the PipelineTask object. This step also demonstrates how to retrieve the output from the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084cd8c-c4ed-4585-9b65-9c2d1e1e36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the component\n",
    "hello_task = say_hello(name=\"Erwin\")\n",
    "\n",
    "# Print the PipelineTask object\n",
    "print(hello_task)\n",
    "\n",
    "# Print the output, which is the greeting text\n",
    "print(hello_task.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e98db-d020-4785-b885-40b96c4c41b1",
   "metadata": {},
   "source": [
    "Step 4: Define the Second Pipeline Component\n",
    "This component (how_are_you) takes the greeting from the first component and appends a follow-up question. It also demonstrates passing the output of one component to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b83e21-d49b-45a4-a195-5528472f2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple example: component 2\n",
    "@dsl.component\n",
    "def how_are_you(hello_text: str) -> str:\n",
    "    how_are_you_text = f\"{hello_text}. How are you?\"  # Append a question\n",
    "    return how_are_you_text  # Return the modified message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db488d0-fa0d-4273-948e-806dcb428071",
   "metadata": {},
   "source": [
    "Step 5: Pass the Output of One Component to Another\n",
    "This step shows how to pass the output from the say_hello component to the how_are_you component and how to handle errors when passing the wrong object type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e1e61-416c-4423-9473-f8bf83598631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the output of say_hello to how_are_you\n",
    "how_task = how_are_you(hello_text=hello_task.output)\n",
    "\n",
    "# Print the PipelineTask object and its output\n",
    "print(how_task)\n",
    "print(how_task.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158f58e-e4e7-4dc5-a37e-059b1461af2c",
   "metadata": {},
   "source": [
    "Step 6: Define the Pipeline\n",
    "This cell defines a simple pipeline (hello_pipeline) that orchestrates the execution of the two components. The pipeline first executes say_hello and then passes its output to how_are_you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb356c-0115-4788-a041-c3a3cd7a7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple example: pipeline\n",
    "@dsl.pipeline\n",
    "def hello_pipeline(recipient: str) -> str:\n",
    "    # First task: say hello\n",
    "    hello_task = say_hello(name=recipient)\n",
    "    \n",
    "    # Second task: ask how the recipient is, using the output from hello_task\n",
    "    how_task = how_are_you(hello_text=hello_task.output)\n",
    "    \n",
    "    # Return the output of how_task\n",
    "    return how_task.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfa9bc-b727-4a8d-9b0c-a63bb6a57425",
   "metadata": {},
   "source": [
    "Step 7: Test the Pipeline\n",
    "This step demonstrates what happens when we attempt to return a PipelineTask object instead of its output. It shows how such an error can occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ea882-d1be-45e3-824a-86e03371d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline with wrong return value type\n",
    "@dsl.pipeline\n",
    "def hello_pipeline_with_error(recipient: str) -> str:\n",
    "    hello_task = say_hello(name=recipient)\n",
    "    how_task = how_are_you(hello_text=hello_task.output)\n",
    "    \n",
    "    # Returning the PipelineTask object instead of the output will give an error\n",
    "    return how_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa2850-5a01-4a61-8296-00616ea619f0",
   "metadata": {},
   "source": [
    "Step 8: Compile the Pipeline\n",
    "This cell compiles the hello_pipeline into a YAML file (pipeline.yaml). The YAML file defines the pipeline structure and is used to execute it in a managed environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf6d11-66e8-4bb6-b58f-b18f471ed01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the pipeline into a YAML file\n",
    "compiler.Compiler().compile(hello_pipeline, 'pipeline.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9feeaa-8852-483f-86ce-c1943edf6d2d",
   "metadata": {},
   "source": [
    "Step 9: Define Pipeline Arguments\n",
    "Define the input arguments that will be passed to the pipeline when it is executed. In this case, we are passing \"World!\" as the recipient for the say_hello component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9554054-2b81-4259-86f6-3d03ec358198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline arguments\n",
    "pipeline_arguments = {\n",
    "    \"recipient\": \"World!\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35bc2f9-1fec-4f39-ac8d-c0f8da16a228",
   "metadata": {},
   "source": [
    "Step 10: View the Generated Pipeline YAML\n",
    "This cell displays the contents of the pipeline.yaml file, which was generated from the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a84d1-74ca-46dd-b4f1-3227ec09f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the generated pipeline.yaml file\n",
    "!cat pipeline.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8b215-712d-4eb5-95c7-4edc1226a455",
   "metadata": {},
   "source": [
    "Step 11: Submit the Pipeline to Vertex AI\n",
    "This cell provides the code needed to submit the pipeline to Vertex AI Pipelines for execution. It submits the pipeline YAML and checks the job status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76e0aa-0e85-4d86-8d93-a4f666a813cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import `PipelineJob`\n",
    "from google.cloud.aiplatform import PipelineJob\n",
    "\n",
    "# Create a PipelineJob object to execute the YAML pipeline\n",
    "job = PipelineJob(\n",
    "    ### Path of the YAML file to execute\n",
    "    template_path=\"pipeline.yaml\",\n",
    "    ### Name of the pipeline\n",
    "    display_name=\"deep_learning_ai_pipeline\",\n",
    "    ### Pipeline arguments (inputs)\n",
    "    parameter_values=pipeline_arguments,\n",
    "    ### Region of execution\n",
    "    location=\"us-central1\",\n",
    "    ### Directory to store temporary files\n",
    "    pipeline_root=\"./\",\n",
    ")\n",
    "\n",
    "# Submit the job for execution\n",
    "job.submit()\n",
    "\n",
    "# Check the job status\n",
    "job.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbbaac8-1e8f-4790-8930-9a1966ff8314",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "This notebook demonstrated the creation of a simple Kubeflow Pipeline. It showed how to define reusable components, create a pipeline, and compile it into a YAML file. The YAML file can then be submitted to Vertex AI Pipelines for managed, serverless execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
