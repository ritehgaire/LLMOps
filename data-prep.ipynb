{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88201de-5760-4bef-bbad-2794b82f472f",
   "metadata": {},
   "source": [
    "Dealing with Data That Lives in Memory\n",
    "When working with large datasets that reside in memory, efficient management and processing become crucial. The vertexai library and SDK provide essential tools for managing data in the cloud. A data warehouse, acting as a central repository, aggregates data from various sources, including application databases, files, and other external data sources. BigQuery, as a serverless data warehouse, allows SQL queries to be used for processing vast amounts of data efficiently.\n",
    "\n",
    "Stack Overflow Dataset\n",
    "The Stack Overflow dataset serves as an illustrative example of such large datasets. It contains tables related to questions, answers, and associated metadata. Users can query and fetch data from this dataset through a notebook interface, writing SQL queries to retrieve data for analysis or fine-tuning machine learning models.\n",
    "\n",
    "Dealing with Large Datasets in Memory\n",
    "Large datasets often cannot fit into memory all at once. SQL queries can be constrained or limited to ensure that the data fits into memory. A 403 error may occur if the dataset exceeds memory limits or access is restricted. Best practices to mitigate this include using data lineage, which tracks the movement and transformation of data. For more efficient storage and access, cloud storage buckets or SSDs can be employed. Another effective solution is data streaming, which processes data in smaller, more manageable batches.\n",
    "\n",
    "Instructions to Large Language Models (LLMs)\n",
    "When providing instructions to Large Language Models (LLMs), specific directives are given to help the model understand the task. For example, the model may be instructed to answer a Stack Overflow question. Rather than looping through the entire dataset, it is more efficient to use a labelled unique ID that allows the model to jump directly to the relevant data. This process improves efficiency, allowing the parser to quickly identify similar labels and produce the corresponding output.\n",
    "\n",
    "Data Formats for Large Datasets\n",
    "Different formats are used to handle large datasets effectively:\n",
    "\n",
    "JSONL (JSON Lines): This format is often used when storing data as rows, such as question-answer pairs. It is easy to parse and widely used in machine learning applications.\n",
    "TFRecord: A binary format designed for efficient reading and writing. It is particularly well-suited for large-scale datasets and is commonly used with TensorFlow models. Its binary nature allows for faster reading and lower memory overhead, making it ideal for machine learning tasks involving large amounts of data.\n",
    "Parquet: A columnar storage format that is highly efficient for complex and large datasets. It provides better performance for reading, querying, and compressing data compared to simpler formats like CSV.\n",
    "Versioning and Artifacts\n",
    "In machine learning workflows, versioning artifacts is essential for reproducibility and traceability. Versioning datasets, models, and other artifacts ensures that the entire process can be repeated or traced back to a specific state. This practice is critical for maintaining control over the different iterations of data, models, and experiments, allowing for robust model development and deployment strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47007f09-3883-465f-b1b9-f5add679aa12",
   "metadata": {},
   "source": [
    "Step 1: Project Environment Setup\n",
    "Load Credentials and Relevant Python Libraries\n",
    "If you were running this notebook locally, you would first install Vertex AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c504d7d-333c-4371-b8af-79cb243b59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The necessary Vertex AI SDK can be installed with the following line if required\n",
    "# !pip install google-cloud-aiplatform\n",
    "from utils import authenticate\n",
    "credentials, PROJECT_ID = authenticate()  # Authenticate and get credentials and project ID\n",
    "REGION = \"us-central1\"  # Set the region for Vertex AI services\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfeda71-8fef-43c3-8ba6-f7bca955d7ea",
   "metadata": {},
   "source": [
    "Step 2: Import Vertex AI SDK and Initialize\n",
    "Import the Vertex AI SDK to interact with the Vertex AI services in the cloud.\n",
    "Initialize the Vertex AI SDK with the project ID, region, and credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe19c11-2b17-4646-a5ea-4789c0767c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, \n",
    "              location=REGION, \n",
    "              credentials=credentials)  # Initialize Vertex AI SDK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13f908-3ade-47bd-b19a-adf80a2e62de",
   "metadata": {},
   "source": [
    "Step 3: Import and Initialize BigQuery Client\n",
    "Import BigQuery to use as your data warehouse.\n",
    "Initialize the BigQuery client to start querying and interacting with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be33a3-3c73-4ddd-965f-ee3e691c70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq_client = bigquery.Client(project=PROJECT_ID, \n",
    "                            credentials=credentials)  # Initialize BigQuery client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8406ca-4ebd-4303-89e6-6333eb75af62",
   "metadata": {},
   "source": [
    "Step 4: Stack Overflow Public Dataset Exploration\n",
    "4.1: Querying for Table Names\n",
    "Use SQL to retrieve the names of all tables from the Stack Overflow public dataset.\n",
    "You will send the query using the BigQuery client and retrieve table names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f85da7-49e3-443e-91d6-46fb61e27688",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TABLES = \"\"\"\n",
    "SELECT\n",
    "  table_name\n",
    "FROM\n",
    "  `bigquery-public-data.stackoverflow.INFORMATION_SCHEMA.TABLES`\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and print the table names\n",
    "query_job = bq_client.query(QUERY_TABLES)\n",
    "for row in query_job:\n",
    "    for value in row.values():\n",
    "        print(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f68e5-b9da-4eda-adb6-2ba9ef9fe633",
   "metadata": {},
   "source": [
    "4.2: Data Retrieval\n",
    "Fetch some data from the data warehouse and store it in a Pandas DataFrame for easier exploration and visualization.\n",
    "Here we will retrieve 3 rows from the posts_questions table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417743ee-021a-4845-9661-743f5010912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSPECT_QUERY = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "# Execute the query and load results into a Pandas DataFrame\n",
    "query_job = bq_client.query(INSPECT_QUERY)\n",
    "stack_overflow_df = query_job.result().to_arrow().to_pandas()  # Convert Arrow table to Pandas DataFrame\n",
    "stack_overflow_df.head()  # Show the first few rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916fa34-042f-45a4-b237-0db5a52fb047",
   "metadata": {},
   "source": [
    "Step 5: Dealing with Large Datasets\n",
    "For large datasets that cannot fit into memory, we handle them carefully by processing them in smaller batches or optimizing queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab05a2-cb04-489e-88b7-9a98d9b7eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_ALL = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "\"\"\"\n",
    "\n",
    "query_job = bq_client.query(QUERY_ALL)\n",
    "try:\n",
    "    stack_overflow_df = query_job.result().to_arrow().to_pandas()  # This will raise an exception if too large\n",
    "except Exception as e:\n",
    "    print('The DataFrame is too large to load into memory.', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3b154-fbb7-4934-af65-8059c97ecbca",
   "metadata": {},
   "source": [
    "Step 6: Joining Tables and Query Optimization\n",
    "6.1: SQL Query to Join Posts and Answers\n",
    "We select questions and answers from the posts_questions and posts_answers tables.\n",
    "Join them based on the unique question ID, filter for Python-related questions, and limit the results to 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed3e56-b05f-4d2c-887a-734b689ef0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"\"\"\n",
    "SELECT\n",
    "    CONCAT(q.title, q.body) as input_text,\n",
    "    a.body AS output_text\n",
    "FROM\n",
    "    `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "JOIN\n",
    "    `bigquery-public-data.stackoverflow.posts_answers` a\n",
    "ON\n",
    "    q.accepted_answer_id = a.id\n",
    "WHERE\n",
    "    q.accepted_answer_id IS NOT NULL AND\n",
    "    REGEXP_CONTAINS(q.tags, \"python\") AND\n",
    "    a.creation_date >= \"2020-01-01\"\n",
    "LIMIT\n",
    "    10000\n",
    "\"\"\"\n",
    "\n",
    "# Execute the optimized query to retrieve data\n",
    "query_job = bq_client.query(QUERY)\n",
    "stack_overflow_df = query_job.result().to_arrow().to_pandas()  # Store results in Pandas DataFrame\n",
    "stack_overflow_df.head(2)  # Display the first two rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65562f1-c6d4-4ef7-966e-080fb5b02ff5",
   "metadata": {},
   "source": [
    "Step 7: Adding Instructions for LLM\n",
    "To improve the performance of the Language Model, we add instructions to the input questions.\n",
    "This helps the model understand what task to perform (i.e., answering a Stack Overflow question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342bd924-7abe-460e-9088-c92434a57a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction template to guide the LLM for better generalization\n",
    "INSTRUCTION_TEMPLATE = \"\"\"\\\n",
    "Please answer the following Stackoverflow question on Python. \\\n",
    "Answer it like you are a developer answering Stackoverflow questions.\n",
    "\n",
    "Stackoverflow question:\n",
    "\"\"\"\n",
    "\n",
    "# Combine the instruction template with the input text (questions)\n",
    "stack_overflow_df['input_text_instruct'] = INSTRUCTION_TEMPLATE + ' ' + stack_overflow_df['input_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c1d10-f5e2-4a6e-bf05-c6400834e530",
   "metadata": {},
   "source": [
    "Step 8: Dataset for Tuning\n",
    "8.1: Splitting Data into Training and Evaluation Sets\n",
    "Divide the data into 80% for training and 20% for evaluation using train_test_split.\n",
    "The random_state ensures the split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f459adb-cb09-4986-92a9-2ebbd1a2de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training (80%) and evaluation (20%) sets\n",
    "train, evaluation = train_test_split(\n",
    "    stack_overflow_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31668b72-43b8-4164-b061-c29210debc20",
   "metadata": {},
   "source": [
    "Step 9: Generating JSONL Files for Training\n",
    "9.1: Data Versioning\n",
    "Versioning is important for reproducibility.\n",
    "Generate a unique file name using the current timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e155382-f13e-4582-b3c9-e1fec990fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")  # Get current date and time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ac7a0-9af1-481f-975a-54a811059a3b",
   "metadata": {},
   "source": [
    "9.2: Creating JSONL File for Model Tuning\n",
    "Save the input questions (with instructions) and corresponding answers in JSONL format for easy use during tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974c78f-f57d-42cf-85b7-efd8a6118b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['input_text_instruct','output_text']  # Columns to include in the JSONL file\n",
    "tune_jsonl = train[cols].to_json(orient=\"records\", lines=True)  # Convert to JSONL format\n",
    "\n",
    "# Generate file name and save the JSONL data\n",
    "training_data_filename = f\"tune_data_stack_overflow_python_qa-{date}.jsonl\"\n",
    "with open(training_data_filename, \"w\") as f:\n",
    "    f.write(tune_jsonl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28b7a3-05f3-4bad-86c4-16118fbabb06",
   "metadata": {},
   "source": [
    "Conclusion: Comprehensive Overview\n",
    "In this notebook, we demonstrated the complete process of setting up a machine learning environment to explore, prepare, and optimize a large dataset for tuning a foundation language model. The focus was on handling large datasets (from a public Stack Overflow dataset) in a scalable and efficient manner, ensuring that even when datasets do not fit into memory, we can process and train on them without issues. Below is a summary of each step:\n",
    "\n",
    "1. Project Environment Setup\n",
    "We initialized the environment by loading necessary credentials and libraries, specifically focusing on Google Cloud Vertex AI and BigQuery to manage large-scale data and machine learning tasks.\n",
    "Vertex AI helps with running and scaling machine learning workflows, while BigQuery allows us to query large datasets efficiently.\n",
    "2. BigQuery Data Access and Exploration\n",
    "We explored the Stack Overflow public dataset hosted on BigQuery, which contains millions of questions, answers, and metadata related to Stack Overflow interactions.\n",
    "Using SQL queries, we retrieved a list of table names and explored the first few rows from the posts_questions table to understand the structure and contents of the data.\n",
    "3. Handling Large Datasets\n",
    "Since the dataset is extremely large (potentially too big to fit into memory), we implemented strategies to manage such data:\n",
    "Query Optimization: We limited our data exploration to manageable chunks using the SQL LIMIT clause.\n",
    "Error Handling: We ensured that if we attempted to load an oversized dataset, the program could gracefully handle it by catching exceptions.\n",
    "4. Joining Tables for Input and Output Creation\n",
    "To prepare data for training a foundation model, we joined the questions (from posts_questions) with their corresponding answers (from posts_answers), filtering the data to focus on questions tagged with \"Python\" and posted after 2020.\n",
    "This step also optimized the query to return only a specific number of rows (10,000), ensuring the retrieval was efficient without overloading memory or computational resources.\n",
    "5. Adding Instructions for Language Model Fine-Tuning\n",
    "We enhanced the dataset by adding instructions for the Language Model. Adding these instructions significantly improves the performance of large language models (LLMs), as it provides explicit context about the task to be performed.\n",
    "The model is prompted to behave like a Stack Overflow developer answering Python-related questions. This approach is based on research that shows LLMs perform better when given clear task-specific instructions.\n",
    "6. Dataset Preparation for Fine-Tuning\n",
    "After preparing the questions and answers (now enhanced with instructions), we divided the dataset into training and evaluation sets, using an 80/20 split.\n",
    "This allows for more data to be used for tuning the model while holding out 20% of the data for evaluation, ensuring that the model's performance is tested on unseen data.\n",
    "7. Data Versioning and Storage\n",
    "Versioning data is crucial for reproducibility, ensuring that experiments can be tracked and models can be re-trained or improved with specific datasets.\n",
    "We generated a timestamped JSONL (JSON Lines) file, which is the ideal format for LLM training tasks. This file contains the input questions (with instructions) and the corresponding answers, ready to be fed into a model for fine-tuning.\n",
    "Key Takeaways:\n",
    "Scalable Data Processing: By leveraging tools like BigQuery and Vertex AI, we were able to handle large-scale datasets efficiently without running into memory limitations. This is crucial for real-world applications, where datasets often exceed local memory capabilities.\n",
    "\n",
    "Query Optimization and Data Retrieval: When working with massive datasets, careful query planning, filtering, and limiting results help retrieve data without overwhelming the system.\n",
    "\n",
    "Instruction-Based Model Tuning: Adding clear instructions to the input data can improve model performance, especially in scenarios where the model needs to generalize to unseen tasks or specific domains (in this case, Python programming questions).\n",
    "\n",
    "Dataset Splitting and Versioning: Splitting the dataset into training and evaluation sets ensures fair testing, and versioning with timestamps helps maintain traceability across different experiments or model versions.\n",
    "\n",
    "Next Steps:\n",
    "The dataset is now ready for fine-tuning a language model. The next steps would include:\n",
    "\n",
    "Uploading the prepared JSONL file to a cloud storage bucket.\n",
    "Training a foundation model using the prepared dataset by fine-tuning it on Vertex AI or another machine learning service.\n",
    "Monitoring and evaluating the model's performance on the evaluation set, and making iterative improvements based on the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
