{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f5d57a-9eb4-46ce-ba43-277da4958562",
   "metadata": {},
   "source": [
    "Real-Life Pipeline Example: Automation and Orchestration of a Supervised Tuning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce89a0f-40d7-4cc0-9299-5e8c05c4faf8",
   "metadata": {},
   "source": [
    "This example demonstrates how to automate the orchestration of a machine learning pipeline using Kubeflow Pipelines for parameter-efficient fine-tuning (PEFT). The pipeline reuses an existing pipeline template from Google for tuning the PaLM 2 foundation model, allowing users to specify parameters without building the pipeline from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072455c-75a7-48a3-9aa8-3bec9700b034",
   "metadata": {},
   "source": [
    "Step 1: Specify Data URIs\n",
    "In this step, we define the training and evaluation data URIs, which point to the dataset files in JSONL format. These files will be used for tuning the model.\n",
    "The data files are consistent across learners, ensuring reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00318a8b-ff21-4cb7-b174-abfdf2c22a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specify the URIs for training and evaluation data.\n",
    "### These are jsonl files that contain the question-answer pairs for tuning.\n",
    "TRAINING_DATA_URI = \"./tune_data_stack_overflow_python_qa.jsonl\" \n",
    "EVAUATION_DATA_URI = \"./tune_eval_data_stack_overflow_python_qa.jsonl\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e6e3c-5c9a-4705-8c62-c32a50d84cd6",
   "metadata": {},
   "source": [
    "Step 2: Provide a Model Version\n",
    "Versioning is important for reproducibility, auditing, and rollback capabilities. By providing a unique version name for the model (using a timestamp), we can track changes and restore previous versions if necessary.\n",
    "This step creates a unique model name with the current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910e1f4-319f-4c70-8b5a-c02a6b4c4031",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import datetime to generate a versioned model name.\n",
    "import datetime\n",
    "\n",
    "### Generate a timestamp for versioning the model.\n",
    "date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")\n",
    "\n",
    "### Create a unique model name with the current date and time.\n",
    "MODEL_NAME = f\"deep-learning-ai-model-{date}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c70b39-39fe-4d7e-ae55-34adfe7f0364",
   "metadata": {},
   "source": [
    "Step 3: Define Model Tuning Parameters\n",
    "In this step, two key parameters are defined:\n",
    "TRAINING_STEPS: The number of training steps for fine-tuning the model. For extractive QA, it is usually between 100-500.\n",
    "EVALUATION_INTERVAL: Defines how frequently the model is evaluated during the training process (in this case, every 20 steps).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3e02f-1c13-4c3e-a48d-179381e8a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the number of training steps for fine-tuning the model.\n",
    "TRAINING_STEPS = 200\n",
    "\n",
    "### Define the evaluation interval to assess the model's performance during training.\n",
    "EVALUATION_INTERVAL = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742193f-1e26-47db-9c20-fa77e69c1093",
   "metadata": {},
   "source": [
    "Step 4: Load Project ID and Credentials\n",
    "This cell loads the necessary credentials and project ID to authenticate and connect with the Google Cloud Platform, which is required for executing the Kubeflow Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadbdf2c-d223-4dc3-8da8-ae787f5723bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the authenticate function to load credentials and project ID.\n",
    "from utils import authenticate\n",
    "\n",
    "### Load credentials and project ID for Google Cloud.\n",
    "credentials, PROJECT_ID = authenticate()\n",
    "\n",
    "### Specify the region for pipeline execution.\n",
    "REGION = \"us-central1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde0712-b77b-4ad8-8c2b-7b3ca07c903c",
   "metadata": {},
   "source": [
    "Step 5: Define Pipeline Arguments\n",
    "Here, we define the pipeline arguments that will be passed to the pipeline when it's executed. These include:\n",
    "model_display_name: The unique name for the model being tuned.\n",
    "location: The region where the pipeline will be executed.\n",
    "large_model_reference: The reference to the foundation model being fine-tuned (PaLM 2 in this case).\n",
    "train_steps: The number of training steps.\n",
    "dataset_uri: The location of the training data.\n",
    "evaluation_interval: The interval for evaluating the model.\n",
    "evaluation_data_uri: The location of the evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e224ad5-86af-4cfd-b5dc-28681c7d1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the input arguments that will be passed to the pipeline.\n",
    "pipeline_arguments = {\n",
    "    \"model_display_name\": MODEL_NAME,  # Versioned model name\n",
    "    \"location\": REGION,  # Execution region\n",
    "    \"large_model_reference\": \"text-bison@001\",  # PaLM 2 model reference\n",
    "    \"project\": PROJECT_ID,  # Google Cloud project ID\n",
    "    \"train_steps\": TRAINING_STEPS,  # Number of training steps\n",
    "    \"dataset_uri\": TRAINING_DATA_URI,  # URI for training data\n",
    "    \"evaluation_interval\": EVALUATION_INTERVAL,  # Evaluation interval\n",
    "    \"evaluation_data_uri\": EVAUATION_DATA_URI,  # URI for evaluation data\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa27d6-5703-4b6f-968c-7bc1792e63eb",
   "metadata": {},
   "source": [
    "Step 6: Specify Pipeline Template Path\n",
    "The pipeline will reuse an existing pipeline template, which simplifies the process. The path to the template is provided here, which allows the pipeline to be executed using pre-defined configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1240d-5798-4563-aeba-0880271868ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specify the path to the pipeline template to reuse.\n",
    "### This template is a pre-configured pipeline for large model tuning.\n",
    "template_path = 'https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496dd9ab-b4cd-47e6-a2dd-b370d32b7806",
   "metadata": {},
   "source": [
    "Step 7: Submit Pipeline Job for Execution\n",
    "This step prepares and submits the pipeline job for execution using PipelineJob.\n",
    "The pipeline job includes:\n",
    "template_path: The YAML file or template that defines the pipeline.\n",
    "display_name: A unique name for the pipeline run.\n",
    "parameter_values: The pipeline arguments (input data and configurations).\n",
    "pipeline_root: A directory where temporary files will be stored during execution.\n",
    "enable_caching: This option enables caching of pipeline components, so only changed components need to be re-executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c1c7b-31cf-440f-8c9e-767e3ab301f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import PipelineJob to create a pipeline execution job.\n",
    "from google.cloud.aiplatform import PipelineJob\n",
    "\n",
    "### Specify the pipeline root directory.\n",
    "pipeline_root = \"./\"\n",
    "\n",
    "### Create a pipeline job with the specified parameters and submit it for execution.\n",
    "job = PipelineJob(\n",
    "        template_path=template_path,  # Path to the pipeline YAML template\n",
    "        display_name=f\"deep_learning_ai_pipeline-{date}\",  # Unique name for the pipeline run\n",
    "        parameter_values=pipeline_arguments,  # Input parameters for the pipeline\n",
    "        location=REGION,  # Execution region\n",
    "        pipeline_root=pipeline_root,  # Root directory for storing intermediate files\n",
    "        enable_caching=True,  # Enable caching to avoid re-running unchanged components\n",
    ")\n",
    "\n",
    "### Submit the pipeline job for execution.\n",
    "job.submit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b118cb-28db-4066-a685-11d1279f6936",
   "metadata": {},
   "source": [
    "Step 8: Check Job Status\n",
    "After submitting the pipeline job, this cell allows us to check the status of the job to ensure it is running as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49d105-25c0-461d-8379-f638c74d8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check the current state of the submitted job.\n",
    "job.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4f290a-6715-4186-8145-b0b0cd4a1d61",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "This pipeline automates the tuning of a foundation model (PaLM 2) using a Kubeflow Pipeline. By reusing an existing pipeline template, we significantly reduce the complexity of building the pipeline from scratch. The pipeline supports important ML tasks like model versioning, training, evaluation, and orchestration, enabling efficient parameter-efficient fine-tuning (PEFT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
